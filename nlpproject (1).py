# -*- coding: utf-8 -*-
"""NLPproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nCHc50h8ftJw3aRssrmAYqKuEsQKqccW
"""

import pandas as pd
import numpy as np
import nltk

df = pd.read_csv('Data.csv', encoding = "ISO-8859-1")
df

df.head()

df.tail()

#dividing the dataset into train and test data
#dividing the data in both by date timeline
train = df[df['Date'] < '20150101']
test = df[df['Date'] > '20141231']

#Removing punctuations(,;:)because they are not required in sentiment analysis
data = train.iloc[:,2:27] 
data.replace("[^a-zA-Z]"," ",regex=True, inplace=True)

#Renaming column names for ease of access
list1= [i for i in range(25)]
new_Index=[str(i) for i in list1]
data.columns= new_Index
data.head(5)

#Converting headlines to lower case
 for index in new_Index:
   data[index]=data[index].str.lower()
data.head(1)

' '.join(str(x) for x in data.iloc[1,0:25])

headlines = []
for row in range(0,len(data.index)):
  headlines.append(' '.join(str(x) for x in data.iloc[row,0:25]))

headlines[3]

#CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier

#implement BAG OF WORDS
countvector=CountVectorizer(ngram_range=(2,2))
traindataset=countvector.fit_transform(headlines)

traindataset[0]

#implement RandomForestClassifier
randomclassifier=RandomForestClassifier(n_estimators=200, criterion='entropy')
randomclassifier.fit(traindataset, train['Label'])

#Predict for the test Dataset
test_transform= []
for row in range(0,len(test.index)):
  test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))
test_dataset = countvector.transform(test_transform)
predictions = randomclassifier.predict(test_dataset)

test.loc[3723,:]

predictions

#Import library to check accuracy
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

matrix=confusion_matrix(test['Label'],predictions)
print(matrix)
score=accuracy_score(test['Label'],predictions)
print(score)
report=classification_report(test['Label'],predictions)
print(report)

#CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier

#implement Bag of Words
tfidfvector=TfidfVectorizer(ngram_range=(2,2))
traindataset=tfidfvector.fit_transform(headlines)

#implement RandomForest Classifier
randomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')
randomclassifier.fit(traindataset,train['Label'])

#predict for the test dataset
test_transform=[]
for row in range(0,len(test.index)):
  test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))
test_dataset = tfidfvector.transform(test_transform)
predictions = randomclassifier.predict(test_dataset)

predictions

#import library to check accuracy
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

matrix=confusion_matrix(test['Label'],predictions)
print(matrix)
score=accuracy_score(test['Label'],predictions)
print(score)
report=classification_report(test['Label'],predictions)
print(report)

from sklearn.naive_bayes import MultinomialNB

traindataset

naive=MultinomialNB()
naive.fit(traindataset,train['Label'])

naive

#predict for the test dataset
test_transform=[]
for row in range(0,len(test.index)):
  test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))
test_dataset = tfidfvector.transform(test_transform)
predictions = naive.predict(test_dataset)

predictions

matrix=confusion_matrix(test['Label'],predictions)
print(matrix)
score=accuracy_score(test['Label'],predictions)
print(score)
report=classification_report(test['Label'],predictions)
print(report)

"""Using LSTM"""

!pip install keras
!pip install --upgrade keras
!pip install --upgrade tensorflow
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize and pad the input sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(headlines)
sequences = tokenizer.texts_to_sequences(headlines)
word_index = tokenizer.word_index
max_sequence_length = 100
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)

# Define the LSTM model
model = Sequential()
model.add(Embedding(len(word_index) + 1, 128, input_length=max_sequence_length))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the LSTM model
model.fit(padded_sequences, train['Label'], epochs=20, validation_split=0.2)

# Tokenize and pad the test sequences
test_sequences = tokenizer.texts_to_sequences(test_transform)
test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)

# Predict on the test data
probabilities = model.predict(test_padded_sequences)
predictions = np.argmax(probabilities, axis=1)

# Evaluate the LSTM model
score = model.evaluate(test_padded_sequences, test['Label'], verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

#Importing Libraries
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import Adam
from keras.layers import Conv1D,MaxPooling1D,Flatten

# Create a sequential model
model = Sequential()

#define
num_words = len(tokenizer.word_index) + 1
# Set the maximum length of a headline
max_len = 100

# Add the embedding layer
model.add(Embedding(input_dim=num_words, output_dim=32, input_length=max_len))

# Add the convolutional layer
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))

# Add the pooling layer
model.add(MaxPooling1D(pool_size=2))

# Flatten the output from the pooling layer
model.add(Flatten())

# Add the dense layers
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

score = model.evaluate(test_padded_sequences, test['Label'], verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

